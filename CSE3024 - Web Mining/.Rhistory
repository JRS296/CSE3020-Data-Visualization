data1<-table(pred1,Testdata$Class)
data1
data <- read.csv("C:/Users/jonat/Downloads/glass.csv", header = T)
View(Glass)
summary(Glass)
nrow(Glass)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(train)
#var<-sample(nrow(HouseVotes84),nrow(HouseVotes84)*.8)
#Traindata<-HouseVotes84[var,]
#Testdata<-HouseVotes84[-var,]
#Building Naive Bayes Model
NB1<-naiveBayes(Type~.,data=Traindata)
#Predicting Test Data
pred1<-predict(NB1,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data1<-table(pred1,Testdata$Class)
data1
#Accuracy from Confusion Matrix
TP<-data1[1]
FN<-data1[2]
FP<-data1[3]
TN<-data1[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
#Building Naive Bayes Model
NB2<-naiveBayes(Type~.,data=Traindata, laplace = 1)
#Predicting Test Data
pred2<-predict(NB2,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data2<-table(pred2,Testdata$Class)
data2
#Accuracy from Confusion Matrix
TP<-data2[1]
FN<-data2[2]
FP<-data2[3]
TN<-data2[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
data <- read.csv("C:/Users/jonat/Downloads/glass.csv", header = T)
View(data)
summary(data)
nrow(data)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(train)
#var<-sample(nrow(HouseVotes84),nrow(HouseVotes84)*.8)
#Traindata<-HouseVotes84[var,]
#Testdata<-HouseVotes84[-var,]
#Building Naive Bayes Model
NB1<-naiveBayes(Type~.,data=Traindata)
#Predicting Test Data
pred1<-predict(NB1,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data1<-table(pred1,Testdata$Class)
data1
#Accuracy from Confusion Matrix
TP<-data1[1]
FN<-data1[2]
FP<-data1[3]
TN<-data1[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
#Building Naive Bayes Model
NB2<-naiveBayes(Type~.,data=Traindata, laplace = 1)
#Predicting Test Data
pred2<-predict(NB2,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data2<-table(pred2,Testdata$Class)
data2
#Accuracy from Confusion Matrix
TP<-data2[1]
FN<-data2[2]
FP<-data2[3]
TN<-data2[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
View(Glass)
data <- read.csv("C:/Users/jonat/Downloads/glass.csv", header = T)
data(Glass,package="mlbench")
View(Glass)
summary(Glass)
nrow(Glass)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(train)
#var<-sample(nrow(HouseVotes84),nrow(HouseVotes84)*.8)
#Traindata<-HouseVotes84[var,]
#Testdata<-HouseVotes84[-var,]
#Building Naive Bayes Model
NB1<-naiveBayes(Type~.,data=Traindata)
#Predicting Test Data
pred1<-predict(NB1,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data1<-table(pred1,Testdata$Class)
data1
#Accuracy from Confusion Matrix
TP<-data1[1]
FN<-data1[2]
FP<-data1[3]
TN<-data1[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
#Building Naive Bayes Model
NB1<-naiveBayes(Type ~.,data=Traindata)
#Building Naive Bayes Model
NB1<-naiveBayes(Type ~.,data=Traindata, type ='class')
data <- read.csv("C:/Users/jonat/Downloads/glass.csv", header = T)
#Building Naive Bayes Model
NB1<-naiveBayes(type ~.,data=Traindata, type ='class')
#Building Naive Bayes Model
NB1<-naiveBayes(type~.,data=Traindata, type ='class')
View(data)
data(Glass,package="mlbench", header = T)
View(data)
str(Glass)
data <- read.csv(Glass,package="mlbench", header = T)
data()
View(Glass)
summary(Glass)
str(Glass)
data <- read.csv(file.choose(), header = T)
str(data)
View(Glass)
data <- Glass
str(data)
data <- Glass
str(data)
View(Glass)
summary(Glass)
data <- Audiology
str(data)
data <- Glass
str(data)
View(Glass)
summary(Glass)
str(Glass)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(Traindata)
#Building Naive Bayes Model
NB1<-naiveBayes(type~.,data=Traindata, type ='class')
data <- Glass
View(Glass)
summary(Glass)
str(data)
xtabs(~Na+Ca, data = data)
data <- Glass
View(Glass)
summary(Glass)
str(data)
xtabs(~Na+Ca, data = data)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(Traindata)
#Visualization
#Building Naive Bayes Model
NB1<-naiveBayes(type~.,data=Traindata, type ='class')
#Predicting Test Data
pred1<-predict(NB1,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data1<-table(pred1,Testdata$Class)
data1
#Accuracy from Confusion Matrix
TP<-data1[1]
FN<-data1[2]
FP<-data1[3]
TN<-data1[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
install.packages("mlbench")
#data<-("C:/Users/jonat/Downloads/glass.csv", Header = T)
#Could not clean and Properly Access audiology.data file, could not locate attribute "decision attribute"
data <- Glass
View(Glass)
summary(Glass)
str(data)
xtabs(~Na+Ca, data = data)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(Traindata)
#Visualization
#Building Naive Bayes Model
NB1<-naiveBayes(type~.,data=Traindata, type ='class')
#Predicting Test Data
pred1<-predict(NB1,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data1<-table(pred1,Testdata$Class)
data1
#Accuracy from Confusion Matrix
TP<-data1[1]
FN<-data1[2]
FP<-data1[3]
TN<-data1[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
data(Glass,package="mlbench")
View(Glass)
summary(Glass)
str(data)
xtabs(~Na+Ca, data = data)
set.seed(1234)
ind <- sample(2, nrow(data), replace=T, prob = c(0.8,0.2))
Traindata <- data[ind == 1,]
Testdata <- data[ind == 2,]
print(Traindata)
#Visualization
#Building Naive Bayes Model
NB1<-naiveBayes(type~.,data=Traindata, type ='class')
#Predicting Test Data
pred1<-predict(NB1,Testdata[,-1])
#Evaluation of the Model: Confusion Matrix
data1<-table(pred1,Testdata$Class)
data1
#Accuracy from Confusion Matrix
TP<-data1[1]
FN<-data1[2]
FP<-data1[3]
TN<-data1[4]
Accuracy<-((TP+TN)/(TP+TN+FP+FN))
Misc<-((FP+FN)/(TP+TN+FP+FN))
Precision<-((TP)/(TP+FP))
Sens<-((TP)/(TP+FN))
Spec<-((TN)/(TN+FP))
print(Accuracy, zero.print = ".")
print(Misc, zero.print = ".")
print(Sens, zero.print = ".")
print(Spec, zero.print = ".")
str(Glass)
xtabs(~Na+Ca, data = Glass)
str(Glass)
xtabs(~Na+K, data = Glass)
var<-sample(nrow(Glass),nrow(Glass)*.8)
var<-sample(nrow(Glass),nrow(Glass)*.1)
var<-sample(nrow(Glass),nrow(Glass)*.8)
# Naive Bayes
# Libraries
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
#Read data file
getwd()
data <- read.csv(file.choose(), header = T)
#contingency table
xtabs(~admit + rank, data = data)
#Rank & admit are categorical variables
data$rank <- as.factor(data$rank)
data$admit <- as.factor(data$admit)
# Visualization
pairs.panels(data[-1])
data %>%
group_by(admit) %>%
ggplot(aes(x=admit, y=gre, fill=admit)) +
geom_boxplot()
data %>%
ggplot(aes(x=admit, y=gpa, fill=admit)) +
geom_boxplot() +
ggtitle('Box Plot')
data %>%
ggplot(aes(x=gre, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
data %>%
ggplot(aes(x=gpa, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
#Split data into Training (80%) and Testing (20%) datasets
set.seed(1234)
ind <- sample(2,nrow(data),replace=TRUE, prob=c(0.8,.2))
train <- data[ind==1,]
test <- data[ind==2,]
# Naive Bayes
model <- naive_bayes(admit ~ ., data = train)
model
plot(model)
# numeric predictors - means (1st col) & sd's (2nd col)
train %>% filter(admit=="0") %>%
summarize(mean(gre), sd(gre))
# Predict
p <- predict(model, train, type= 'prob')
head(cbind(p, train))
# Misclassification error - train data
p1 <- predict(model, train)
(tab1 <- table(p1, train$admit))
1 - sum(diag(tab1))/ sum(tab1)
# Misclassification error - test data
p2 <- predict(model, test)
(tab2 <- table(p2, test$admit))
1 - sum(diag(tab2))/ sum(tab2)
#Read data file
data <- ("C:/Users/jonat/Downloads/binary.csv")
#contingency table
xtabs(~admit + rank, data = data)
#Rank & admit are categorical variables
data$rank <- as.factor(data$rank)
data$admit <- as.factor(data$admit)
# Visualization
pairs.panels(data[-1])
data %>%
group_by(admit) %>%
ggplot(aes(x=admit, y=gre, fill=admit)) +
geom_boxplot()
data %>%
ggplot(aes(x=admit, y=gpa, fill=admit)) +
geom_boxplot() +
ggtitle('Box Plot')
data %>%
ggplot(aes(x=gre, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
data %>%
ggplot(aes(x=gpa, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
#Split data into Training (80%) and Testing (20%) datasets
set.seed(1234)
ind <- sample(2,nrow(data),replace=TRUE, prob=c(0.8,.2))
train <- data[ind==1,]
test <- data[ind==2,]
# Naive Bayes
model <- naive_bayes(admit ~ ., data = train)
model
plot(model)
# numeric predictors - means (1st col) & sd's (2nd col)
train %>% filter(admit=="0") %>%
summarize(mean(gre), sd(gre))
# Predict
p <- predict(model, train, type= 'prob')
head(cbind(p, train))
# Misclassification error - train data
p1 <- predict(model, train)
(tab1 <- table(p1, train$admit))
1 - sum(diag(tab1))/ sum(tab1)
# Misclassification error - test data
p2 <- predict(model, test)
(tab2 <- table(p2, test$admit))
1 - sum(diag(tab2))/ sum(tab2)
# Naive Bayes
# Libraries
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
#Read data file
data <- ("C:/Users/jonat/Downloads/binary.csv")
#contingency table
xtabs(~admit + rank, data = data)
#Rank & admit are categorical variables
data$rank <- as.factor(data$rank)
data$admit <- as.factor(data$admit)
# Visualization
pairs.panels(data[-1])
data %>%
group_by(admit) %>%
ggplot(aes(x=admit, y=gre, fill=admit)) +
geom_boxplot()
data %>%
ggplot(aes(x=admit, y=gpa, fill=admit)) +
geom_boxplot() +
ggtitle('Box Plot')
data %>%
ggplot(aes(x=gre, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
data %>%
ggplot(aes(x=gpa, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
#Split data into Training (80%) and Testing (20%) datasets
set.seed(1234)
ind <- sample(2,nrow(data),replace=TRUE, prob=c(0.8,.2))
train <- data[ind==1,]
test <- data[ind==2,]
# Naive Bayes
model <- naive_bayes(admit ~ ., data = train, laplace = 1)
model
plot(model)
# numeric predictors - means (1st col) & sd's (2nd col)
train %>% filter(admit=="0") %>%
summarize(mean(gre), sd(gre))
# Predict
p <- predict(model, train, type= 'prob')
head(cbind(p, train))
# Misclassification error - train data
p1 <- predict(model, train)
(tab1 <- table(p1, train$admit))
1 - sum(diag(tab1))/ sum(tab1)
# Misclassification error - test data
p2 <- predict(model, test)
(tab2 <- table(p2, test$admit))
1 - sum(diag(tab2))/ sum(tab2)
# Naive Bayes
# Libraries
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
#Read data file
data <- ("C:/Users/jonat/Downloads/binary.csv")
#contingency table
xtabs(~admit + rank, data = data)
#Rank & admit are categorical variables
data$rank <- as.factor(data$rank)
data$admit <- as.factor(data$admit)
# Visualization
pairs.panels(data[-1])
data %>%
group_by(admit) %>%
ggplot(aes(x=admit, y=gre, fill=admit)) +
geom_boxplot()
data %>%
ggplot(aes(x=admit, y=gpa, fill=admit)) +
geom_boxplot() +
ggtitle('Box Plot')
data %>%
ggplot(aes(x=gre, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
data %>%
ggplot(aes(x=gpa, fill=admit)) +
geom_density(alpha=0.8, color='black') +
ggtitle('Density Plot')
#Split data into Training (80%) and Testing (20%) datasets
set.seed(1234)
ind <- sample(2,nrow(data),replace=TRUE, prob=c(0.8,.2))
train <- data[ind==1,]
test <- data[ind==2,]
# Naive Bayes
model <- naive_bayes(admit ~ ., data = train, laplace = 1)
model
plot(model)
# numeric predictors - means (1st col) & sd's (2nd col)
train %>% filter(admit=="0") %>%
summarize(mean(gre), sd(gre))
# Predict
p <- predict(model, train, type= 'prob')
head(cbind(p, train))
# Misclassification error - train data
p1 <- predict(model, train)
(tab1 <- table(p1, train$admit))
1 - sum(diag(tab1))/ sum(tab1)
# Misclassification error - test data
p2 <- predict(model, test)
(tab2 <- table(p2, test$admit))
1 - sum(diag(tab2))/ sum(tab2)
# Libraries
library(naivebayes)
var<-sample(nrow(HouseVotes84),nrow(HouseVotes84)*.8)
data(Glass,package="mlbench")
